# Formula1 Data Processing

In the Formula 1 Data Processing project, I have developed an ETL pipeline using PySpark and Azure Databricks to process raw data stored in CSV and JSON formats in Azure Data Lake. This resulted in the creation of target parquet files, which were then stored in Delta Lake as tables. The pipeline was scheduled using Azure Data Factory to ensure efficient data processing. The processed data was then used to create dashboards and perform advanced analysis on drivers, constructors, and race results. 
This project required skills in Python, Spark SQL, ADLS Gen2, Azure Data Factory, Azure Databricks, and Delta Lake, and GitHub was used for version control.
